{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# SENTIMENT CLASSIFICATION PROJECT OVERVIEW\\n\\n    In this project, a labelled test set is provided with text snippets labelled as either \"Positive\" or \"Negative\". \\n    The model is trained based on this input data to predict the sentiments of the text snippets in the test set. The \\n    test set provided has numeric ids and text snippets, which is used for predicting the outcome. The output will \\n    contain the id and predictions (positive or negative).\\n\\n### Participants\\n    Baburaj Velayudhan\\n    bvelayudhan@vmware.com\\n    CMBU\\n\\n### Summary   \\n    The input text data is cleaned to remove any @mentions, URLs, hashtags, punctuations, html etc and a train-test split \\n    is created for validating the trained model. Using a wordcloud, the most common words in positive and negative \\n    sentiments are identified. There are many common words in positive and negative sentiments.\\n\\n    I have used MultinomialNB with CountVectorizer and TfidfVectorizer. To model accuracy is compared with a LogisticRegression\\n    model with CountVectorizer and TfidfVectorizer. Based on my trials, i have selected MultinomialNB trigram model for \\n    final predictions.\\n   \\n### Feature Selection\\n    For feature selection, chi2 with SelectKBest with chi2 is used. The chi2 score is highest with 1707 features at 76.57% \\n    accuracy for trigrams using Tfidf vectorizer.\\n   \\n### Training methodology\\n    MultinomialNB with CountVectorizer and TfidfVectorizer:\\n   \\n    As a first step, CountVectorizer is used with MulinomialNB classifier on unigrams, bigrams and trigrams. The model \\n    accuracy is computed for feature count ranging from 1 to 10000 with and without Words. A trigram model showed high \\n    accuracy with stop words. A classification report for bigram and trigrams indicate almost same accuracy of 75.53 % \\n    and f1- score of 70 for Negative and 79 for Positive cases. The process is repeated with TfidfVectorizer and \\n    MultinomialNB and observed that a trigram model wth tfidf had 76.58% accuracy with 1901 features\\n   \\n    LogisticRegression with CountVectorizer and TfidfVectorizer:\\n       The same process is repeated with LogisticRegression model and it shows that the bi-gram and tri-gram accuracy drops \\n       from the previous trials.\\n\\n    With the above, MultinomialNB was chosen with Tfidf maximum features of 1901 and a SelectKBest showed 76.57% accuracy \\n    with 1707 features. The classification report showed 73% accuracy for negative and 79% accuracy for positive cases. \\n    Hence this was chosen for predictions.\\n\\n### Notable aspects\\n    Many terms were common in positive and negative sentiments. I believe, if such words were filtered out, the model accuracy\\n    would have improved a bit more.\\n    \\n### References\\n    The data cleaning and model training approach is highly infiuenced by the Ricky Kim\\'s article published in \\n    https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\\n   \\n### Assumptions\\n    (1) The input data file is located in the current folder and filename is Export_loop-sentiment-pos-neg-train_05112020000000.csv\\n    (b) The test data file is located in the current folder and the file name is sentiment-eval.csv\\n    (c) The final predictions will be located in the current folder and the file name is preditions.csv\\n        \\n### python and library versions\\n        Python        : 3.7.6\\n        sklearn       : 0.22.1\\n        numpy         : 1.18.1\\n        seaborn       : 0.10.0\\n        Beautiful Soup: 4.8.2\\n        Matplotlib    : 3.1.3\\n        pandas        : 1.0.1\\n        wordcloud     : 1.7.0\\n        NLTK          : 3.4.5\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# SENTIMENT CLASSIFICATION PROJECT OVERVIEW\n",
    "\n",
    "    In this project, a labelled test set is provided with text snippets labelled as either \"Positive\" or \"Negative\". \n",
    "    The model is trained based on this input data to predict the sentiments of the text snippets in the test set. The \n",
    "    test set provided has numeric ids and text snippets, which is used for predicting the outcome. The output will \n",
    "    contain the id and predictions (positive or negative).\n",
    "\n",
    "### Participants\n",
    "    Baburaj Velayudhan\n",
    "    bvelayudhan@vmware.com\n",
    "    CMBU\n",
    "\n",
    "### Summary   \n",
    "    The input text data is cleaned to remove any @mentions, URLs, hashtags, punctuations, html etc and a train-test split \n",
    "    is created for validating the trained model. Using a wordcloud, the most common words in positive and negative \n",
    "    sentiments are identified. There are many common words in positive and negative sentiments.\n",
    "\n",
    "    I have used MultinomialNB with CountVectorizer and TfidfVectorizer. To model accuracy is compared with a LogisticRegression\n",
    "    model with CountVectorizer and TfidfVectorizer. Based on my trials, i have selected MultinomialNB trigram model for \n",
    "    final predictions.\n",
    "   \n",
    "### Feature Selection\n",
    "    For feature selection, chi2 with SelectKBest with chi2 is used. The chi2 score is highest with 1707 features at 76.57% \n",
    "    accuracy for trigrams using Tfidf vectorizer.\n",
    "   \n",
    "### Training methodology\n",
    "    MultinomialNB with CountVectorizer and TfidfVectorizer:\n",
    "   \n",
    "    As a first step, CountVectorizer is used with MulinomialNB classifier on unigrams, bigrams and trigrams. The model \n",
    "    accuracy is computed for feature count ranging from 1 to 10000 with and without Words. A trigram model showed high \n",
    "    accuracy with stop words. A classification report for bigram and trigrams indicate almost same accuracy of 75.53 % \n",
    "    and f1- score of 70 for Negative and 79 for Positive cases. The process is repeated with TfidfVectorizer and \n",
    "    MultinomialNB and observed that a trigram model wth tfidf had 76.58% accuracy with 1901 features\n",
    "   \n",
    "    LogisticRegression with CountVectorizer and TfidfVectorizer:\n",
    "       The same process is repeated with LogisticRegression model and it shows that the bi-gram and tri-gram accuracy drops \n",
    "       from the previous trials.\n",
    "\n",
    "    With the above, MultinomialNB was chosen with Tfidf maximum features of 1901 and a SelectKBest showed 76.57% accuracy \n",
    "    with 1707 features. The classification report showed 73% accuracy for negative and 79% accuracy for positive cases. \n",
    "    Hence this was chosen for predictions.\n",
    "\n",
    "### Notable aspects\n",
    "    Many terms were common in positive and negative sentiments. I believe, if such words were filtered out, the model accuracy\n",
    "    would have improved a bit more.\n",
    "    \n",
    "### References\n",
    "    The data cleaning and model training approach is highly infiuenced by the Ricky Kim's article published in \n",
    "    https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "   \n",
    "### Assumptions\n",
    "    (1) The input data file is located in the current folder and filename is Export_loop-sentiment-pos-neg-train_05112020000000.csv\n",
    "    (b) The test data file is located in the current folder and the file name is sentiment-eval.csv\n",
    "    (c) The final predictions will be located in the current folder and the file name is preditions.csv\n",
    "        \n",
    "### python and library versions\n",
    "        Python        : 3.7.6\n",
    "        sklearn       : 0.22.1\n",
    "        numpy         : 1.18.1\n",
    "        seaborn       : 0.10.0\n",
    "        Beautiful Soup: 4.8.2\n",
    "        Matplotlib    : 3.1.3\n",
    "        pandas        : 1.0.1\n",
    "        wordcloud     : 1.7.0\n",
    "        NLTK          : 3.4.5\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input data from \"data\" subfolder\n",
    "\n",
    "FILE_NAME = \"./Export_loop-sentiment-pos-neg-train_05112020000000.csv\"\n",
    "data = pd.read_csv(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape\n",
      "(1900, 2)\n"
     ]
    }
   ],
   "source": [
    "#get some insights \n",
    "print(\"Data set shape\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall info on Data set\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1900 entries, 0 to 1899\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   1900 non-null   object\n",
      " 1   text    1900 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 29.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# get some insights\n",
    "print(\"Overall info on Data set\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique value of labels in Data set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Negative', 'Positive'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distinct values in label\n",
    "print(\"Unique value of labels in Data set\")\n",
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct label values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Positive    1013\n",
       "Negative     887\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the counts of labels in the set\n",
    "print(\"Count of distinct label values\")\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set has no null values and have 1900 rows in total.\n",
    "# distinct values in label are 'positive' and 'negative'\n",
    "\n",
    "# Out of 1900, 1013 are positive and 887 are negative,\n",
    "# this shows that the data is not skewed to positive or negative sentiment\n",
    "\n",
    "# The 'text' neeeds some cleaning to remove any @mentions\n",
    "# urls, punctuations, white spaces, unicode bytemarks\n",
    "# hashtags, numbers etc.\n",
    "\n",
    "# after cleaning the dataset, do a train-test split to \n",
    "# validate the accuracy of the models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data for remove any\n",
    "# @ mentions\n",
    "# html\n",
    "# puntuations\n",
    "# urls\n",
    "# numbers\n",
    "# hashtags\n",
    "# unicode bytemarks\n",
    "# \n",
    "\n",
    "token = WordPunctTokenizer()\n",
    "at = r'@[A-Za-z0-9_]+'\n",
    "url = r'https?://[^ ]+'\n",
    "combined = r'|'.join((at, url))\n",
    "www_pat = r'www.[^ ]+'\n",
    "\n",
    "# negations will lose meaning if the \"'\" is removed as part of cleaning. so \n",
    "# make a dictionary with the common negations and replace the dictionary values \n",
    "# if any dictionary key is found in text\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \n",
    "                \"aren't\":\"are not\", \n",
    "                \"wasn't\":\"was not\", \n",
    "                \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\n",
    "                \"hasn't\":\"has not\",\n",
    "                \"hadn't\":\"had not\",\n",
    "                \"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \n",
    "                \"don't\":\"do not\", \n",
    "                \"doesn't\":\"does not\",\n",
    "                \"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\n",
    "                \"couldn't\":\"could not\",\n",
    "                \"shouldn't\":\"should not\",\n",
    "                \"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\", \n",
    "                \"you're\": \"you are\", \n",
    "                \"you'll\":\"you will\",\n",
    "                \"we'll\":\"we will\", \n",
    "                \"we've\": \"we have\", \n",
    "                \"you've\": \"you have\",\n",
    "                \"i'm\" : \"i am\"}\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def clean_text(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined, '', souped)\n",
    "    \n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    # remove url pattens, change to lower case, replace negation words with dictionary values,\n",
    "    # replace anything except alphabets and remove extra spaces\n",
    "    \n",
    "    clean = re.sub(combined, '', clean)\n",
    "    clean = re.sub(www_pat, '', clean)\n",
    "    clean = clean.lower()\n",
    "    clean = neg_pattern.sub(lambda x: negations_dic[x.group()], clean)\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    \n",
    "    words = [x for x  in token.tokenize(clean) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "data['label'] = data['label'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>no one cares about marketing slides technical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>are all three hosts providing storage capacity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>would loved to had managed to get down to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>vending machine at work is out of dasani water...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>rt paul maritz ceo and president of vmware is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  negative  no one cares about marketing slides technical ...\n",
       "1  positive  are all three hosts providing storage capacity...\n",
       "2  negative  would loved to had managed to get down to the ...\n",
       "3  negative  vending machine at work is out of dasani water...\n",
       "4  positive  rt paul maritz ceo and president of vmware is ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaned Data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordCloud representation for negative sentiments\n"
     ]
    }
   ],
   "source": [
    "# Get a word cloud representation for positive and negative comments\n",
    "# and get a word frequency for positive and negatve comments.\n",
    "\n",
    "print(\"WordCloud representation for negative sentiments\")\n",
    "\n",
    "negative = data[data.label ==\"negative\"]\n",
    "\n",
    "neg_string = []\n",
    "for t in negative.text:\n",
    "    neg_string.append(t)\n",
    "    \n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WordCloud representation for positive sentiments\")\n",
    "\n",
    "positive = data[data.label ==\"positive\"]\n",
    "\n",
    "pos_string = []\n",
    "for t in positive.text:\n",
    "    pos_string.append(t)\n",
    "    \n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(pos_string)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some of the words appear in both the positive and negative sentiments. for example, vmware, work, host etc.\")\n",
    "print(\"Words such as  still, got, work, issue,day etc appear in negative sentiments a lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" The words such as just, like, vmware, does, need etc appear in both the positive and negative sentiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a split for testing the model accuracy.\n",
    "# reserve 20% as test data. \n",
    "\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size = 0.2, stratify=y)\n",
    "\n",
    "X_train_DF = pd.concat([X_train, y_train], axis = 1)\n",
    "X_test_DF = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "train_shape = X_train_DF.shape\n",
    "train_row, train_col = (train_shape)\n",
    "\n",
    "test_shape = X_test_DF.shape\n",
    "test_row, test_col = (test_shape)\n",
    "print(\"There are {train_row} rows in training set and {test_row} rows in test set\".format(train_row=train_row,test_row=test_row))\n",
    "\n",
    "print(\"Count of distinct label values in train data\")\n",
    "print(X_train_DF['label'].value_counts())\n",
    "\n",
    "print(\"Count of distinct label values in test data\")\n",
    "print(X_test_DF['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now start with CountVectorizer with stop word elimination and\n",
    "# get a prediction done with MulinomialNB classifier\n",
    "\n",
    "#CountVectorizer with stop word elimination\n",
    "cvect = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_dtm = cvect.fit_transform(X_train)\n",
    "X_test_dtm = cvect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = mnb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy without stop words\n",
    "accur_wo_sw = mnb.score(X_test_dtm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model accuracy without stop words is {accuracy_score} \".format(accuracy_score=accur_wo_sw *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the null accuracy\n",
    "\n",
    "y_test_num = y_test.to_frame()\n",
    "\n",
    "y_test_num['label'] =  y_test_num['label'].astype(\"category\")\n",
    "y_test_num['labelNum'] =  y_test_num['label'].cat.codes\n",
    "\n",
    "null_accuracy = max(y_test_num['labelNum'].mean(),1-y_test_num['labelNum'].mean())\n",
    "print(\"The null accuracy is {null_accuracy}\".format(null_accuracy=null_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model without stop word elimination\n",
    "\n",
    "cvect = CountVectorizer()\n",
    "X_train_dtm = cvect.fit_transform(X_train)\n",
    "X_test_dtm = cvect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = mnb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "accur_with_sw =mnb.score(X_test_dtm,y_test)\n",
    "print(\"The model accuracy with stop words is {accuracy_score} and the null accuracy is {null_accuracy}\".format(accuracy_score=accur_with_sw*100,null_accuracy=null_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that with with stop words, the model has a better accuracy than the one without stop words\n",
    "# Now identify the ideal number of features by iterating over a number of features \n",
    "# and visualize accuracy in a chart.\n",
    "\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def nfeature_accuracy_checker(n_features, stop_words, ngram_range, classifier, vectorizer):\n",
    "    result = []\n",
    "    \n",
    "    accuracy_score = 0\n",
    "    feature_count = 0\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        if nfeature_accuracy > accuracy_score:\n",
    "            accuracy_score = nfeature_accuracy\n",
    "            feature_count = n\n",
    "            \n",
    "        result.append((n,nfeature_accuracy))\n",
    "    \n",
    "    print (\"Validation result for {} features has the highest accuracy, {}\".format(feature_count, accuracy_score* 100))\n",
    "    return result\n",
    "\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "mnb = MultinomialNB()\n",
    "n_features = np.arange(1,10000,100)\n",
    "\n",
    "\n",
    "#get the unigram MultinomialNB accuracy for features ranging from 1 to 10000\n",
    "print (\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
    "feature_accuracy_sw = nfeature_accuracy_checker(vectorizer=cvec, classifier=mnb, ngram_range=(1,1), n_features = n_features, stop_words= None)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"\")\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
    "feature_accuracy_wo_sw = nfeature_accuracy_checker(vectorizer=cvec, classifier=mnb,stop_words='english',ngram_range=(1,1), n_features = n_features)\n",
    "\n",
    "\n",
    "#DF of unigram model wit stop words\n",
    "feature_ug_sw_DF = pd.DataFrame(feature_accuracy_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of unigram model without stop words\n",
    "feature_ug_wo_sw_DF = pd.DataFrame(feature_accuracy_wo_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(feature_ug_sw_DF.feature_count, feature_ug_sw_DF.model_accuracy, label='with stop words')\n",
    "plt.plot(feature_ug_wo_sw_DF.feature_count, feature_ug_wo_sw_DF.model_accuracy,label='without stop words')\n",
    "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The plot indicates that \")\n",
    "print(\"          The unigram model accuracy is high with stop words\")\n",
    "print(\"          The accuracy is 75.26% stop words for 1101 features\")\n",
    "print(\"          The accuracy is 72.36%  without stop words for 3301 features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the accuracy for bi-gram and tri-gram models with and without stop words\n",
    "\n",
    "# set an initial number of features to 100000\n",
    "n_features = np.arange(1,100000,100)\n",
    "\n",
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\")\n",
    "accuracy_sw_bi_gram = nfeature_accuracy_checker(stop_words=None,vectorizer=cvec, classifier=mnb,n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR TRIGRAM WITH STOP WORDS\")\n",
    "accuracy_sw_tri_gram = nfeature_accuracy_checker(stop_words=None,vectorizer=cvec, classifier=mnb,n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR BIGRAM WITHOUT STOP WORDS\")\n",
    "accuracy_wo_sw_bi_gram = nfeature_accuracy_checker(stop_words='english',vectorizer=cvec, classifier=mnb,n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT STOP WORDS\")\n",
    "accuracy_wo_sw_tri_gram = nfeature_accuracy_checker(stop_words='english',vectorizer=cvec, classifier=mnb,n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model accuracies \n",
    "\n",
    "#DF of bigram model with stop words\n",
    "sw_bi_gram_DF = pd.DataFrame(accuracy_sw_bi_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model with stop words\n",
    "sw_tri_gram_DF = pd.DataFrame(accuracy_sw_tri_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of bigram model without stop words\n",
    "wo_sw_bi_gram_DF = pd.DataFrame(accuracy_wo_sw_bi_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model without stop words\n",
    "wo_sw_tri_gram_DF = pd.DataFrame(accuracy_wo_sw_tri_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(sw_bi_gram_DF.feature_count, sw_bi_gram_DF.model_accuracy, label='bi-gram with stop words')\n",
    "plt.plot(sw_tri_gram_DF.feature_count, sw_tri_gram_DF.model_accuracy,label='tri-gram with stop words')\n",
    "plt.plot(wo_sw_bi_gram_DF.feature_count, wo_sw_bi_gram_DF.model_accuracy,label='bi-gram without stop words')\n",
    "plt.plot(wo_sw_tri_gram_DF.feature_count, wo_sw_tri_gram_DF.model_accuracy,label='tri-gram without stop words')\n",
    "\n",
    "\n",
    "plt.title(\"Bi-gram VS Tri-gram: Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The plot indicates that tri-gram model with stopwords has the highest accuracy at 75.526% for 9901 features\")\n",
    "print(\"Though the bi-gram model with stopwords has the same accuracy, the number of features are 10701\")      \n",
    "print(\"Now, get the confusion matrix and classification report for bi-gram and tri-gram model for 10701 and 9901 features respectievely\")\n",
    "\n",
    "\n",
    "def get_classif_report(pipeline, x_train, y_train, x_test, y_test):\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy = {accuracy_score}\".format(accuracy_score=accuracy))\n",
    "    print (metrics.classification_report(y_test, y_pred, target_names=['negative','positive']))\n",
    "    \n",
    "print(\"Bigram Classification Report\")\n",
    "bigram_CV = CountVectorizer(max_features=10701,ngram_range=(1, 2))\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', bigram_CV),\n",
    "        ('classifier', mnb)\n",
    "    ])\n",
    "\n",
    "get_classif_report(pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"-------------------------\\n\")\n",
    "print(\"Trigram Classification Report\")\n",
    "trigram_CV = CountVectorizer(max_features=9901,ngram_range=(1, 3))\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', trigram_CV),\n",
    "        ('classifier', mnb)\n",
    "    ])\n",
    "\n",
    "get_classif_report(pipeline, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using CountVectorizer, the tri-gram model with stop words and 9901 features has same accuracy as bigram model without stop words and 10701 features\")\n",
    "print(\"Both the models have f1 score of 70% for negative and 79% for positive cases\")\n",
    "\n",
    "print(\"Now ,try a TFIdf Vectorizer and compare the accuracy to see if the accuracy is better with or without stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "n_features = np.arange(1,10000,1000)\n",
    "\n",
    "print(\"\\n\")\n",
    "print (\"RESULT FOR UNIGRAM WITH STOP WORDS (TFIDF)\")\n",
    "tfidf_ug_sw = nfeature_accuracy_checker(vectorizer=tvec, classifier=mnb, n_features=n_features, stop_words= None, ngram_range=(1,1))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (TFIDF) \\n\")\n",
    "tfidf_ug_wo_sw = nfeature_accuracy_checker(vectorizer=tvec, classifier=mnb, n_features=n_features, stop_words= 'english', ngram_range=(1,1))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "#DF of unigram model without stop words\n",
    "tfidf_ug_sw_DF = pd.DataFrame(tfidf_ug_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of unigram model with english stop words\n",
    "tfidf_ug_wo_sw_DF = pd.DataFrame(tfidf_ug_wo_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(tfidf_ug_sw_DF.feature_count, tfidf_ug_sw_DF.model_accuracy, label='with stop words')\n",
    "plt.plot(tfidf_ug_wo_sw_DF.feature_count, tfidf_ug_wo_sw_DF.model_accuracy,label='without stop words')\n",
    "plt.title(\"TFIDF Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"With TfidfVectorizer, the unigram model accuracy is high without stop words (73.94%) with 3001 features\")\n",
    "print( \"which is less than what we achieved (75%)with countVectorizer unigram\")\n",
    "\n",
    "print(\" Now try bigram and trigram model with TFIDF\")\n",
    "\n",
    "\n",
    "# Now get the accuracy for bi-gram and tri-gram models with and without stop words\n",
    "\n",
    "# set an initial number of features to 100000\n",
    "n_features = np.arange(1,100000,100)\n",
    "\n",
    "print (\"RESULT FOR BIGRAM WITH STOP WORDS\")\n",
    "tfidf_accuracy_sw_bi_gram = nfeature_accuracy_checker(stop_words=None,vectorizer=tvec, classifier=mnb,n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR TRIGRAM WITH STOP WORDS\")\n",
    "tfidf_accuracy_sw_tri_gram = nfeature_accuracy_checker(stop_words=None,vectorizer=tvec, classifier=mnb,n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR BIGRAM WITHOUT STOP WORDS\")\n",
    "tfidf_accuracy_wo_sw_bi_gram = nfeature_accuracy_checker(stop_words='english',vectorizer=tvec, classifier=mnb,n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR TRIGRAM WITHOUT STOP WORDS\")\n",
    "tfidf_accuracy_wo_sw_tri_gram = nfeature_accuracy_checker(stop_words='english',vectorizer=tvec, classifier=mnb,n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "#DF of bigram model with stop words\n",
    "tfidf_sw_bi_gram_DF = pd.DataFrame(tfidf_accuracy_sw_bi_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model with stop words\n",
    "tfidf_sw_tri_gram_DF = pd.DataFrame(tfidf_accuracy_sw_tri_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of bigram model without stop words\n",
    "tfidf_wo_sw_bi_gram_DF = pd.DataFrame(tfidf_accuracy_wo_sw_bi_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model without stop words\n",
    "tfidf_wo_sw_tri_gram_DF = pd.DataFrame(tfidf_accuracy_wo_sw_tri_gram, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(tfidf_sw_bi_gram_DF.feature_count, tfidf_sw_bi_gram_DF.model_accuracy, label='bi-gram with stop words')\n",
    "plt.plot(tfidf_sw_tri_gram_DF.feature_count, tfidf_sw_tri_gram_DF.model_accuracy,label='tri-gram with stop words')\n",
    "plt.plot(tfidf_wo_sw_bi_gram_DF.feature_count, tfidf_wo_sw_bi_gram_DF.model_accuracy,label='bi-gram without stop words')\n",
    "plt.plot(tfidf_wo_sw_tri_gram_DF.feature_count, tfidf_wo_sw_tri_gram_DF.model_accuracy,label='tri-gram without stop words')\n",
    "\n",
    "\n",
    "plt.title(\"TFIDF Bi-gram VS Tri-gram: Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With TFIDF, a MultinomialNB bigram model with stop words gave accuracy of 76.05 with 2301 features\")\n",
    "print(\"With TFIDF, a MultinomialNB trigram model with stop words gave accuracy of 76.578 with 1901 features\")\n",
    "print(\"With TFIDF, a MultinomialNB trigram model without stop words gave accuracy of 71.842 with 3001 features\")\n",
    "print(\"With TFIDF, a MultinomialNB trigram model without stop words gave accuracy of 71.842 with 2901 features\")\n",
    "\n",
    "print(\"Try a LogisticRegression model with TFIDF to see if the accuracy improves\")\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "n_features = np.arange(1,10000,100)\n",
    "\n",
    "print(\"RESULT FOR UNIGRAM WITH STOP WORDS (TFIDF)\")\n",
    "tfidf_ug_lr_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words=None,n_features=n_features,ngram_range=(1, 1))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (TFIDF)\")\n",
    "tfidf_ug_lr_wo_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words='english',n_features=n_features,ngram_range=(1, 1))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "#DF of unigram model without stop words\n",
    "tfidf_ug_lr_sw_DF = pd.DataFrame(tfidf_ug_lr_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of unigram model with english stop words\n",
    "tfidf_ug_lr_wo_sw_DF = pd.DataFrame(tfidf_ug_lr_wo_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(tfidf_ug_lr_sw_DF.feature_count, tfidf_ug_lr_sw_DF.model_accuracy, label='with stop words')\n",
    "plt.plot(tfidf_ug_lr_wo_sw_DF.feature_count, tfidf_ug_lr_wo_sw_DF.model_accuracy,label='without stop words')\n",
    "plt.title(\"LogisticRegression Model Without stop words VS With stop words (Unigram): Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now try bigram and trigram model LogisticRegression to see if we get a better accuracy\n",
    "\n",
    "n_features = np.arange(1,100000,10000)\n",
    "\n",
    "\n",
    "print (\"RESULT FOR LOGISTIC REGRESSION BIGRAM WITH STOP WORDS\\n\")\n",
    "tfidf_lr_bigram_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words=None,n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR LOGISTIC REGRESSION BIGRAM WITHOUT STOP WORDS\\n\")\n",
    "tfidf_lr_bigram_wo_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words='english',n_features=n_features,ngram_range=(1, 2))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "                                                   \n",
    "print (\"RESULT FOR LOGISTIC REGRESSION TRIGRAM WITH STOP WORDS\\n\")\n",
    "tfidf_lr_trigram_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words=None,n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "print (\"RESULT FOR LOGISTIC REGRESSION TRIGRAM WITH STOP WORDS\\n\")\n",
    "tfidf_lr_trigram_wo_sw = nfeature_accuracy_checker(classifier=lr,vectorizer=tvec,stop_words='english',n_features=n_features,ngram_range=(1, 3))\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "#DF of bigram model with stop words\n",
    "tfidf_lr_bigram_sw_DF = pd.DataFrame(tfidf_lr_bigram_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of bigram model without stop words\n",
    "tfidf_lr_bigram_wo_sw_DF = pd.DataFrame(tfidf_lr_bigram_wo_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model with stop words\n",
    "tfidf_lr_trigram_sw_DF = pd.DataFrame(tfidf_lr_trigram_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "#DF of trigram model without stop words\n",
    "tfidf_lr_trigram_wo_sw_DF = pd.DataFrame(tfidf_lr_trigram_wo_sw, columns = ['feature_count','model_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(tfidf_lr_bigram_sw_DF.feature_count, tfidf_lr_bigram_sw_DF.model_accuracy, label='bi-gram with stop words')\n",
    "plt.plot(tfidf_lr_bigram_wo_sw_DF.feature_count, tfidf_lr_bigram_wo_sw_DF.model_accuracy,label='bi-gram without stop words')\n",
    "plt.plot(tfidf_lr_trigram_sw_DF.feature_count, tfidf_lr_trigram_sw_DF.model_accuracy, label='tri-gram with stop words')\n",
    "plt.plot(tfidf_lr_trigram_wo_sw_DF.feature_count, tfidf_lr_trigram_wo_sw_DF.model_accuracy,label='tri-gram without stop words')\n",
    "\n",
    "plt.title(\"LogisticRegression Model Bigram and Trigram: Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" The accuracy of LR unigram is 76.84% with 801 features\")\n",
    "print(\" The accuracy of LR trigram is 74.47% with 10001 features\")\n",
    "print(\" The accuracy is less than that of MultinomialNB trigam, which is 76.578 with 1901 features\")\n",
    "\n",
    "print(\" Lets use MultinomialNB trigram model with max features of 1901 and use Chi2 to pick the most relevant features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use chi2 method to get the most relevant features out of the 1901\n",
    "tfidf = TfidfVectorizer(max_features = 1901, ngram_range = (1,3))\n",
    "\n",
    "x_train_tfidf = tfidf.fit_transform(X_train)\n",
    "x_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "#get the chi2 values\n",
    "chisq = chi2(x_train_tfidf, y_train)[0]\n",
    "\n",
    "#to see the top features based on chi2 values, combine feature names and associated chi2 values and \n",
    "#create a data frame and sort on chi2 desceding order.\n",
    "feature_DF = pd.DataFrame({\"Feature_Name\": tfidf.get_feature_names(), \"Chisq_Value\" : chisq.tolist()})\n",
    "\n",
    "\n",
    "feature_DF.sort_values( by= \"Chisq_Value\", ascending= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pick the best features \n",
    "\n",
    "high_score = 0\n",
    "feature_count = 0\n",
    "\n",
    "ch2_result = []\n",
    "for n in np.arange(1,1901,1):\n",
    "    ch2 = SelectKBest(chi2, k=n)\n",
    "    x_train_k_best = ch2.fit_transform(x_train_tfidf, y_train)\n",
    "    x_test_k_best = ch2.transform(x_test_tfidf)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train_k_best, y_train)\n",
    "    score = clf.score(x_test_k_best, y_test)\n",
    "    if score > high_score:\n",
    "        high_score = score\n",
    "        feature_count = n\n",
    "    ch2_result.append(score)\n",
    "    # print (\"chi2 feature selection evaluation calculated for {} features and the score is {}\".format(n,score))\n",
    "       \n",
    "#print(\"The K Best Features are selected and the Feature count is 7001 with score of 75%\")\n",
    "print(\"The high score is {} with feature count {}\".format(high_score* 100,feature_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final Model\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=1707)\n",
    "x_train_k_best = ch2.fit_transform(x_train_tfidf, y_train)\n",
    "x_test_k_best = ch2.transform(x_test_tfidf)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_k_best, y_train)\n",
    "score = clf.score(x_test_k_best, y_test)\n",
    "print(\"Accuracy score of the final model is {}\".format(score*100))    \n",
    "\n",
    "\n",
    "y_pred = clf.predict(x_test_k_best)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy = {accuracy_score}\".format(accuracy_score=accuracy*100))\n",
    "nt (metrics.classification_report(y_test, y_pred, target_names=['negative','positive']))\n",
    "\n",
    "print(\"\\n The f1-score is 73% for negative and 79% for positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the evaluation set from data subfolder\n",
    "\n",
    "FILE_NAME = \"./sentiment-eval.csv\"\n",
    "eval_data = pd.read_csv(FILE_NAME, header=0)\n",
    "\n",
    "# do the data cleaning on eval data set \n",
    "eval_data['text'] = eval_data['text'].apply(clean_text)\n",
    "\n",
    "#TFIDF transform eval data\n",
    "x_eval_tfidf = tfidf.transform(eval_data['text'])\n",
    "x_eval_k_best = ch2.transform(x_eval_tfidf)\n",
    "\n",
    "#predict \n",
    "predictions = clf.predict(x_eval_k_best)\n",
    "\n",
    "\n",
    "pred_df = pd.DataFrame(predictions, columns=['result'])\n",
    "\n",
    "eval_data['label'] = pred_df['result'].values\n",
    "\n",
    "eval_data = eval_data.drop('text',axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "\n",
    "eval_data.to_csv('./predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
